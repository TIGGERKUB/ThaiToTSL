{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SignLanguage2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/H+sg60LKWm+2gznWwC2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TIGGERKUB/ThaiToTSL/blob/main/SignLanguage2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-LFP9CalfFR"
      },
      "source": [
        "# !pip install sklearn_crfsuite\r\n",
        "# !pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\r\n",
        "# !pip install fastai==1.0.46\r\n",
        "# !pip install attacut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVM7SLWnkQ9z"
      },
      "source": [
        "import numpy as np\r\n",
        "from keras.models import Model, load_model\r\n",
        "from keras.layers import Input, LSTM, Dense,Embedding\r\n",
        "from keras.utils import *\r\n",
        "from keras.initializers import *\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from attacut import tokenize, Tokenizer\r\n",
        "import tensorflow as tf\r\n",
        "import time, random\r\n",
        "import pandas as pd\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV1SAGJdkeB-"
      },
      "source": [
        "from pythainlp.tokenize import word_tokenize\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from pythainlp import word_vector\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.font_manager as fm\r\n",
        "\r\n",
        "import dill as pickle\r\n",
        "import pandas as pd\r\n",
        "model_path = 'thwiki_data/models/'\r\n",
        "model = word_vector.get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5enqjK8MkhT5"
      },
      "source": [
        "#Dimensionality\r\n",
        "dimensionality = 256\r\n",
        "#The batch size and number of epochs\r\n",
        "batch_size = 256\r\n",
        "epochs = 100\r\n",
        "EMBEDDING_SIZE = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vicsg8DlV2V"
      },
      "source": [
        "#create dataframe\r\n",
        "thai2dict = {}\r\n",
        "for word in model.index2word:\r\n",
        "    thai2dict[word] = model[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLu1U9pIlYiR"
      },
      "source": [
        "def read_data(filename):\r\n",
        "  df = pd.read_excel(filename)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjMvxQnmla5D"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/tsl/train_data.xlsx\"\r\n",
        "df = read_data(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a2-odgqlcM3"
      },
      "source": [
        "pairs = list(zip(df['th tokenized'],df['tsl tokenized']))\r\n",
        "random.shuffle(pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGP4JYE7ldyy"
      },
      "source": [
        "input_docs = []\r\n",
        "target_docs = []\r\n",
        "input_tokens = set()\r\n",
        "target_tokens = set()\r\n",
        "for ele in pairs:\r\n",
        "  input_doc, target_doc = ele[0], ele[1]\r\n",
        "  # Appending each input sentence to input_docs\r\n",
        "  input_docs.append(input_doc)\r\n",
        "  # Redefine target_doc below and append it to target_docs\r\n",
        "  target_doc = '<START> ' + target_doc + ' <END>'\r\n",
        "  target_docs.append(target_doc)\r\n",
        "  # Now we split up each sentence into words and add each unique word to our vocabulary set\r\n",
        "  for token in input_doc.split():\r\n",
        "    if token not in input_tokens:\r\n",
        "      input_tokens.add(token)\r\n",
        "  for token in target_doc.split():\r\n",
        "    if token not in target_tokens:\r\n",
        "      target_tokens.add(token)\r\n",
        "input_tokens = sorted(list(input_tokens))\r\n",
        "target_tokens = sorted(list(target_tokens))\r\n",
        "num_encoder_tokens = len(input_tokens)\r\n",
        "num_decoder_tokens = len(target_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk_0D4OylkJa"
      },
      "source": [
        "print('Number of samples:', len(input_docs))\r\n",
        "print('Number of target samples:', len(target_docs))\r\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\r\n",
        "print('Number of unique output tokens:', num_decoder_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyLszeE7lmgY"
      },
      "source": [
        "input_features_dict = dict([(token, i) for i, token in enumerate(input_tokens)])\r\n",
        "target_features_dict = dict([(token, i) for i, token in enumerate(target_tokens)])\r\n",
        "reverse_input_features_dict = dict((i, token) for token, i in input_features_dict.items())\r\n",
        "reverse_target_features_dict = dict((i, token) for token, i in target_features_dict.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFIQwMjIloVl"
      },
      "source": [
        "#Maximum length of sentences in input and target documents\r\n",
        "max_encoder_seq_length = max([len(input_doc.split()) for input_doc in input_docs])\r\n",
        "max_decoder_seq_length = max([len(target_doc.split()) for target_doc in target_docs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSUdEcrSlp8w"
      },
      "source": [
        "encoder_input_data = np.zeros((len(input_docs), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\r\n",
        "decoder_input_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\r\n",
        "decoder_target_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\r\n",
        "\r\n",
        "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\r\n",
        "  for timestep, token in enumerate(input_doc.split()):\r\n",
        "    #Assign 1. for the current line, timestep, & word in encoder_input_data\r\n",
        "    encoder_input_data[line, timestep, input_features_dict[token]] = 1.\r\n",
        "  for timestep, token in enumerate(target_doc.split()):\r\n",
        "    decoder_input_data[line, timestep, target_features_dict[token]] = 1.\r\n",
        "    if timestep > 0:\r\n",
        "      decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlS8uF_Jlrrb"
      },
      "source": [
        "#Encoder\r\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\r\n",
        "encoder_lstm = LSTM(dimensionality, return_state=True)\r\n",
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\r\n",
        "encoder_states = [state_hidden, state_cell]\r\n",
        "#Decoder\r\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\r\n",
        "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\r\n",
        "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\r\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\r\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P9KilDLltEF"
      },
      "source": [
        "#Model\r\n",
        "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\r\n",
        "#Compiling\r\n",
        "training_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\r\n",
        "#Training\r\n",
        "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9olN4Coelu1p"
      },
      "source": [
        "training_model.save('training_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1REO-l9ylw7h"
      },
      "source": [
        "training_model = load_model('training_model.h5')\r\n",
        "encoder_inputs = training_model.input[0]\r\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\r\n",
        "encoder_states = [state_h_enc, state_c_enc]\r\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTaBLPzdlz3w"
      },
      "source": [
        "latent_dim = 256\r\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\r\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\r\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjitYgzZl1gU"
      },
      "source": [
        "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\r\n",
        "decoder_states = [state_hidden, state_cell]\r\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QusutPrNl3lc"
      },
      "source": [
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjDFtvXzl5V3"
      },
      "source": [
        "def decode_response(test_input):\r\n",
        "    #Getting the output states to pass into the decoder\r\n",
        "    states_value = encoder_model.predict(test_input)\r\n",
        "    #Generating empty target sequence of length 1\r\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\r\n",
        "    #Setting the first token of target sequence with the start token\r\n",
        "    target_seq[0, 0, target_features_dict['<START>']] = 1.\r\n",
        "    \r\n",
        "    #A variable to store our response word by word\r\n",
        "    decoded_sentence = ''\r\n",
        "    \r\n",
        "    stop_condition = False\r\n",
        "    while not stop_condition:\r\n",
        "      #Predicting output tokens with probabilities and states\r\n",
        "      output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\r\n",
        "      #Choosing the one with highest probability\r\n",
        "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\r\n",
        "      sampled_token = reverse_target_features_dict[sampled_token_index]\r\n",
        "      decoded_sentence += \" \" + sampled_token#Stop if hit max length or found the stop token\r\n",
        "      if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\r\n",
        "        stop_condition = True\r\n",
        "      #Update the target sequence\r\n",
        "      target_seq = np.zeros((1, 1, num_decoder_tokens))\r\n",
        "      target_seq[0, 0, sampled_token_index] = 1.\r\n",
        "      #Update states\r\n",
        "      states_value = [hidden_state, cell_state]\r\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq4dIiePl8Kn"
      },
      "source": [
        "class Translator:\r\n",
        "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\r\n",
        "  \r\n",
        "  #Method to start the translator\r\n",
        "  def start(self):\r\n",
        "    user_response = input(\"Give in an sentence. :) \\n\")\r\n",
        "    self.translate(user_response)\r\n",
        "  \r\n",
        "  #Method to handle the conversation\r\n",
        "  def translate(self, reply):\r\n",
        "    while not self.make_exit(reply):\r\n",
        "      reply = input(self.generate_response(reply)+\"\\n\")\r\n",
        "  #Method to convert user input into a matrix\r\n",
        "  def string_to_matrix(self, user_input):\r\n",
        "    atta = Tokenizer(model=\"attacut-sc\")\r\n",
        "    words = atta.tokenize(user_input)\r\n",
        "    tokens = words\r\n",
        "    user_input_matrix = np.zeros((1, max_encoder_seq_length, num_encoder_tokens),dtype='float32')\r\n",
        "    for timestep, token in enumerate(tokens):\r\n",
        "      if token in input_features_dict:\r\n",
        "        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\r\n",
        "    return user_input_matrix\r\n",
        "  \r\n",
        "  #Method that will create a response using seq2seq model we built\r\n",
        "  def generate_response(self, user_input):\r\n",
        "    input_matrix = self.string_to_matrix(user_input)\r\n",
        "    chatbot_response = decode_response(input_matrix)\r\n",
        "    #Remove <START> and <END> tokens from chatbot_response\r\n",
        "    chatbot_response = chatbot_response.replace(\"<START>\",'')\r\n",
        "    chatbot_response = chatbot_response.replace(\"<END>\",'')\r\n",
        "    return chatbot_response\r\n",
        "  \r\n",
        "  #Method to check for exit commands\r\n",
        "  def make_exit(self, reply):\r\n",
        "    for exit_command in self.exit_commands:\r\n",
        "      if exit_command in reply:\r\n",
        "        print(\"Ok, have a great day!\")\r\n",
        "        return True\r\n",
        "    return False\r\n",
        "  \r\n",
        "translator = Translator()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}